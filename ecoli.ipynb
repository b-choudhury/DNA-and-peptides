{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Promoter+Gene+Sequences)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data processing\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/promoter-gene-sequences/promoters.data'\n",
    "names = ['Class', 'id', 'Sequence']\n",
    "data = pd.read_csv(url, names = names, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=data['Sequence']\n",
    "classes=data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = data['Class']\n",
    "y_prev=[]\n",
    "y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0,classes.shape[0],1):\n",
    "    y_prev.append(classes[i][0])\n",
    "    if y_prev[i]=='+':\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[3]=data['id'][3]\n",
    "sequences[4]=data['id'][4]\n",
    "sequences[5]=data['id'][5]\n",
    "sequences[9]=data['id'][9]\n",
    "sequences[10]=data['id'][10]\n",
    "sequences[11]=data['id'][11]\n",
    "sequences[12]=data['id'][12]\n",
    "sequences[13]=data['id'][13]\n",
    "sequences[14]=data['id'][14]\n",
    "sequences[15]=data['id'][15]\n",
    "sequences[16]=data['id'][16]\n",
    "sequences[23]=data['id'][23]\n",
    "sequences[27]=data['id'][27]\n",
    "sequences[28]=data['id'][28]\n",
    "sequences[30]=data['id'][30]\n",
    "sequences[31]=data['id'][31]\n",
    "sequences[32]=data['id'][32]\n",
    "sequences[35]=data['id'][35]\n",
    "sequences[40]=data['id'][40]\n",
    "sequences[42]=data['id'][42]\n",
    "sequences[44]=data['id'][44]\n",
    "sequences[45]=data['id'][45]\n",
    "sequences[48]=data['id'][48]\n",
    "sequences[49]=data['id'][49]\n",
    "sequences[52]=data['id'][52]\n",
    "\n",
    "lines=np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert a DNA sequence string to a numpy array\n",
    "# converts to lower case, changes any non 'acgt' characters to 'n'\n",
    "#https://www.kaggle.com/thomasnelson/working-with-dna-sequence-data-for-ml\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "def string_to_array(my_string):\n",
    "    my_string = my_string.lower()\n",
    "    my_string = re.sub('[^acgt]', 'z', my_string)\n",
    "    my_array = np.array(list(my_string))\n",
    "    return my_array\n",
    "\n",
    "# create a label encoder with 'acgtn' alphabet\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.array(['a','c','g','t','z']))\n",
    "\n",
    "def ordinal_encoder(my_array):\n",
    "    integer_encoded = label_encoder.transform(my_array)\n",
    "    float_encoded = integer_encoded.astype(float)\n",
    "    float_encoded[float_encoded == 0] = 0.25 # A\n",
    "    float_encoded[float_encoded == 1] = 0.50 # C\n",
    "    float_encoded[float_encoded == 2] = 0.75 # G\n",
    "    float_encoded[float_encoded == 3] = 1.00 # T\n",
    "    float_encoded[float_encoded == 4] = 0.00 # anything else, z\n",
    "    return float_encoded\n",
    "\n",
    "X=[ordinal_encoder(string_to_array(i)) for i in lines]\n",
    "X=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svc',\n",
       "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto', kernel='rbf', max_iter=-1, probability=False,\n",
       "                     random_state=None, shrinking=True, tol=0.001,\n",
       "                     verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions=(clf.predict(X_test))\n",
    "test_accuracy = np.mean(test_predictions==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.76744186046511"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolutional Neural network\n",
    "\n",
    "import numpy as np\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "\n",
    "import keras\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "# convert class labels to one-hot encoded, should have shape (?, NUM_CLASSES)\n",
    "y_train2 = keras.utils.to_categorical(y_train)\n",
    "y_test2 = keras.utils.to_categorical(y_test)\n",
    "# import necessary building blocks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D,  Flatten, Dense, Activation, Dropout,BatchNormalization,LSTM\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    \"\"\"\n",
    "    Define your model architecture here.\n",
    "    Returns `Sequential` model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(input_shape=(1,57),padding=\"same\",kernel_size=3,filters=16))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv1D(padding=\"same\",kernel_size=3,filters=32))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(padding=\"same\",kernel_size=3,filters=32))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv1D(padding=\"same\",kernel_size=3,filters=64))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 1, 16)             2752      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_116 (LeakyReLU)  (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_78 (Batc (None, 1, 16)             64        \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 1, 32)             1568      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_117 (LeakyReLU)  (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_79 (Batc (None, 1, 32)             128       \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 1, 32)             3104      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_118 (LeakyReLU)  (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_80 (Batc (None, 1, 32)             128       \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 1, 64)             6208      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_119 (LeakyReLU)  (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_81 (Batc (None, 1, 64)             256       \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_120 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_121 (LeakyReLU)  (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 31,362\n",
      "Trainable params: 31,074\n",
      "Non-trainable params: 288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "Learning rate: 0.0045\n",
      "Learning rate: 0.00405\n",
      "Learning rate: 0.003645\n",
      "Learning rate: 0.0032805\n",
      "Learning rate: 0.00295245\n",
      "Learning rate: 0.002657205\n",
      "Learning rate: 0.0023914846\n",
      "Learning rate: 0.002152336\n",
      "Learning rate: 0.0019371024\n",
      "Learning rate: 0.0017433922\n",
      "Learning rate: 0.0015690529\n",
      "Learning rate: 0.0014121477\n",
      "Learning rate: 0.001270933\n",
      "Learning rate: 0.0011438397\n",
      "Learning rate: 0.0010294557\n",
      "Learning rate: 0.0009265101\n",
      "Learning rate: 0.0008338591\n",
      "Learning rate: 0.0007504732\n",
      "Learning rate: 0.00067542586\n",
      "Learning rate: 0.00060788327\n",
      "Learning rate: 0.00054709497\n",
      "Learning rate: 0.0004923855\n",
      "Learning rate: 0.0004431469\n",
      "Learning rate: 0.00039883223\n",
      "Learning rate: 0.000358949\n",
      "Learning rate: 0.0003230541\n",
      "Learning rate: 0.0002907487\n",
      "Learning rate: 0.00026167382\n",
      "Learning rate: 0.00023550644\n",
      "Learning rate: 0.00021195579\n",
      "Learning rate: 0.00019076021\n",
      "Learning rate: 0.0001716842\n",
      "Learning rate: 0.00015451577\n",
      "Learning rate: 0.0001390642\n",
      "Learning rate: 0.00012515778\n",
      "Learning rate: 0.000112641996\n",
      "Learning rate: 0.000101377795\n",
      "Learning rate: 9.124002e-05\n",
      "Learning rate: 8.2116014e-05\n",
      "Learning rate: 7.390441e-05\n",
      "Learning rate: 6.6513974e-05\n",
      "Learning rate: 5.9862577e-05\n",
      "Learning rate: 5.387632e-05\n",
      "Learning rate: 4.8488688e-05\n",
      "Learning rate: 4.363982e-05\n",
      "Learning rate: 3.9275837e-05\n",
      "Learning rate: 3.5348254e-05\n",
      "Learning rate: 3.1813426e-05\n",
      "Learning rate: 2.8632085e-05\n",
      "Learning rate: 2.5768875e-05\n",
      "Learning rate: 2.3191988e-05\n",
      "Learning rate: 2.0872789e-05\n",
      "Learning rate: 1.8785511e-05\n",
      "Learning rate: 1.6906959e-05\n",
      "Learning rate: 1.5216264e-05\n",
      "Learning rate: 1.3694637e-05\n",
      "Learning rate: 1.2325174e-05\n",
      "Learning rate: 1.1092656e-05\n",
      "Learning rate: 9.983391e-06\n",
      "Learning rate: 8.985052e-06\n",
      "Learning rate: 8.086547e-06\n",
      "Learning rate: 7.2778917e-06\n",
      "Learning rate: 6.5501026e-06\n",
      "Learning rate: 5.8950923e-06\n",
      "Learning rate: 5.3055833e-06\n",
      "Learning rate: 4.7750245e-06\n",
      "Learning rate: 4.297522e-06\n",
      "Learning rate: 3.86777e-06\n",
      "Learning rate: 3.480993e-06\n",
      "Learning rate: 3.1328937e-06\n",
      "Learning rate: 2.8196043e-06\n",
      "Learning rate: 2.537644e-06\n",
      "Learning rate: 2.2838794e-06\n",
      "Learning rate: 2.0554917e-06\n",
      "Learning rate: 1.8499425e-06\n",
      "Learning rate: 1.6649482e-06\n",
      "Learning rate: 1.4984533e-06\n",
      "Learning rate: 1.3486081e-06\n",
      "Learning rate: 1.2137472e-06\n",
      "Learning rate: 1.0923725e-06\n",
      "Learning rate: 9.831352e-07\n",
      "Learning rate: 8.848217e-07\n",
      "Learning rate: 7.963396e-07\n",
      "Learning rate: 7.167056e-07\n",
      "Learning rate: 6.45035e-07\n",
      "Learning rate: 5.8053155e-07\n",
      "Learning rate: 5.224784e-07\n",
      "Learning rate: 4.7023053e-07\n",
      "Learning rate: 4.232075e-07\n",
      "Learning rate: 3.8088675e-07\n",
      "Learning rate: 3.4279807e-07\n",
      "Learning rate: 3.0851825e-07\n",
      "Learning rate: 2.7766643e-07\n",
      "Learning rate: 2.4989978e-07\n",
      "Learning rate: 2.2490981e-07\n",
      "Learning rate: 2.0241883e-07\n",
      "Learning rate: 1.8217695e-07\n",
      "Learning rate: 1.6395926e-07\n",
      "Learning rate: 1.4756333e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a47f5ce50>"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "INIT_LR = 5e-3  # initial learning rate\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "# don't call K.set_learning_phase() !!! (otherwise will enable dropout in train/test simultaneously)\n",
    "model = make_model()  # define our model\n",
    "\n",
    "# prepare model for fitting (loss, optimizer, etc)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',  # we train 10-way classification\n",
    "    optimizer=keras.optimizers.adamax(lr=INIT_LR),  # for SGD\n",
    "    metrics=['accuracy']  # report accuracy during training\n",
    ")\n",
    "\n",
    "# scheduler of learning rate (decay with epochs)\n",
    "def lr_scheduler(epoch):\n",
    "    return INIT_LR * 0.9 ** epoch\n",
    "\n",
    "# callback for printing of actual learning rate used by optimizer\n",
    "class LrHistory(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        print(\"Learning rate:\", K.get_value(model.optimizer.lr))\n",
    "\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train, y_train2,  # prepared data\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[keras.callbacks.LearningRateScheduler(lr_scheduler), LrHistory()],\n",
    "    validation_data=(X_test, y_test2),\n",
    "    shuffle=True,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict_proba(X_test).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answers = y_test2.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = np.mean(test_predictions==test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.74418604651163 %\n"
     ]
    }
   ],
   "source": [
    "print(test_accuracy*100,'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
