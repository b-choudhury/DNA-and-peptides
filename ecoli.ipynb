{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Promoter+Gene+Sequences)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data processing\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/promoter-gene-sequences/promoters.data'\n",
    "names = ['Class', 'id', 'Sequence']\n",
    "data = pd.read_csv(url, names = names, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=data['Sequence']\n",
    "classes=data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = data['Class']\n",
    "y_prev=[]\n",
    "y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(0,classes.shape[0],1):\n",
    "    y_prev.append(classes[i][0])\n",
    "    if y_prev[i]=='+':\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[3]=data['id'][3]\n",
    "sequences[4]=data['id'][4]\n",
    "sequences[5]=data['id'][5]\n",
    "sequences[9]=data['id'][9]\n",
    "sequences[10]=data['id'][10]\n",
    "sequences[11]=data['id'][11]\n",
    "sequences[12]=data['id'][12]\n",
    "sequences[13]=data['id'][13]\n",
    "sequences[14]=data['id'][14]\n",
    "sequences[15]=data['id'][15]\n",
    "sequences[16]=data['id'][16]\n",
    "sequences[23]=data['id'][23]\n",
    "sequences[27]=data['id'][27]\n",
    "sequences[28]=data['id'][28]\n",
    "sequences[30]=data['id'][30]\n",
    "sequences[31]=data['id'][31]\n",
    "sequences[32]=data['id'][32]\n",
    "sequences[35]=data['id'][35]\n",
    "sequences[40]=data['id'][40]\n",
    "sequences[42]=data['id'][42]\n",
    "sequences[44]=data['id'][44]\n",
    "sequences[45]=data['id'][45]\n",
    "sequences[48]=data['id'][48]\n",
    "sequences[49]=data['id'][49]\n",
    "sequences[52]=data['id'][52]\n",
    "\n",
    "lines=np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert a DNA sequence string to a numpy array\n",
    "# converts to lower case, changes any non 'acgt' characters to 'n'\n",
    "#https://www.kaggle.com/thomasnelson/working-with-dna-sequence-data-for-ml\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "def string_to_array(my_string):\n",
    "    my_string = my_string.lower()\n",
    "    my_string = re.sub('[^acgt]', 'z', my_string)\n",
    "    my_array = np.array(list(my_string))\n",
    "    return my_array\n",
    "\n",
    "# create a label encoder with 'acgtn' alphabet\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.array(['a','c','g','t','z']))\n",
    "\n",
    "def ordinal_encoder(my_array):\n",
    "    integer_encoded = label_encoder.transform(my_array)\n",
    "    float_encoded = integer_encoded.astype(float)\n",
    "    float_encoded[float_encoded == 0] = 0.25 # A\n",
    "    float_encoded[float_encoded == 1] = 0.50 # C\n",
    "    float_encoded[float_encoded == 2] = 0.75 # G\n",
    "    float_encoded[float_encoded == 3] = 1.00 # T\n",
    "    float_encoded[float_encoded == 4] = 0.00 # anything else, z\n",
    "    return float_encoded\n",
    "\n",
    "X=[ordinal_encoder(string_to_array(i)) for i in lines]\n",
    "X=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svc',\n",
       "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto', kernel='rbf', max_iter=-1, probability=False,\n",
       "                     random_state=None, shrinking=True, tol=0.001,\n",
       "                     verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions=(clf.predict(X_test))\n",
    "test_accuracy = np.mean(test_predictions==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.81818181818183"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolutional Neural network\n",
    "\n",
    "import numpy as np\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "\n",
    "import keras\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "# convert class labels to one-hot encoded, should have shape (?, NUM_CLASSES)\n",
    "y_train2 = keras.utils.to_categorical(y_train)\n",
    "y_test2 = keras.utils.to_categorical(y_test)\n",
    "# import necessary building blocks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D,  Flatten, Dense, Activation, Dropout,BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    \"\"\"\n",
    "    Define your model architecture here.\n",
    "    Returns `Sequential` model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(input_shape=(1,57),padding=\"same\",kernel_size=3,filters=16))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv1D(padding=\"same\",kernel_size=3,filters=32))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(padding=\"same\",kernel_size=3,filters=32))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv1D(padding=\"same\",kernel_size=3,filters=64))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_22 (Conv1D)           (None, 1, 16)             2752      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 1, 16)             64        \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 1, 32)             1568      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 1, 32)             128       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1, 32)             3104      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 1, 32)             128       \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 1, 64)             6208      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)   (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 1, 64)             256       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 31,362\n",
      "Trainable params: 31,074\n",
      "Non-trainable params: 288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Learning rate: 0.005\n",
      "Learning rate: 0.0045\n",
      "Learning rate: 0.00405\n",
      "Learning rate: 0.003645\n",
      "Learning rate: 0.0032805\n",
      "Learning rate: 0.00295245\n",
      "Learning rate: 0.002657205\n",
      "Learning rate: 0.0023914846\n",
      "Learning rate: 0.002152336\n",
      "Learning rate: 0.0019371024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a37a9bd10>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "INIT_LR = 5e-3  # initial learning rate\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "# don't call K.set_learning_phase() !!! (otherwise will enable dropout in train/test simultaneously)\n",
    "model = make_model()  # define our model\n",
    "\n",
    "# prepare model for fitting (loss, optimizer, etc)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',  # we train 10-way classification\n",
    "    optimizer=keras.optimizers.adamax(lr=INIT_LR),  # for SGD\n",
    "    metrics=['accuracy']  # report accuracy during training\n",
    ")\n",
    "\n",
    "# scheduler of learning rate (decay with epochs)\n",
    "def lr_scheduler(epoch):\n",
    "    return INIT_LR * 0.9 ** epoch\n",
    "\n",
    "# callback for printing of actual learning rate used by optimizer\n",
    "class LrHistory(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        print(\"Learning rate:\", K.get_value(model.optimizer.lr))\n",
    "\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train, y_train2,  # prepared data\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[keras.callbacks.LearningRateScheduler(lr_scheduler), LrHistory()],\n",
    "    validation_data=(X_test, y_test2),\n",
    "    shuffle=True,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict_proba(X_test).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answers = y_test2.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = np.mean(test_predictions==test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.81818181818183 %\n"
     ]
    }
   ],
   "source": [
    "print(test_accuracy*100,'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
